{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Pytorch.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["YOifJyv_S8d3","tHCidGr8EnIn","QuQ18rFmH5-P","qg-peL2pKxUz","_sfvmfHKSAIp","CXhmNvMwX7EY","KOiTt-TciTPB"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"I41KgTd_dE9Y","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"qcr3Cb4nPIwU","colab_type":"text"},"cell_type":"markdown","source":["# Learn And implement Pytorch Part - 1\n","---\n","\n","## Kaggle competitions\n","\n","![alt text](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/02/pytorch-logo-flat-300x210.png)\n","\n","#### Important Links\n","* https://towardsdatascience.com/a-beginners-tutorial-on-building-an-ai-image-classifier-using-pytorch-6f85cb69cba7\n","\n","* https://www.kaggle.com/carloalbertobarbano/vgg16-transfer-learning-pytorch\n","* http://scikit-image.org/docs/dev/user_guide/numpy_images.html\n","\n","\n","*By Pankaj Pundir*"]},{"metadata":{"id":"s5fVxTr_DAFy","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"UqsH6b7pC-8b","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"YOifJyv_S8d3","colab_type":"text"},"cell_type":"markdown","source":["# google drive access"]},{"metadata":{"id":"UjzrTuzFfP1Z","colab_type":"code","colab":{}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import os\n","os.chdir(\"drive/My Drive/Colab Notebooks/pyTorch\")\n","\n","# installing pytorch and torchvision\n","!pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl \n","!pip3 install torchvision\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tzfKRYcsOfC_","colab_type":"text"},"cell_type":"markdown","source":["# Import **Libraries**"]},{"metadata":{"id":"v3RGqYa_fqGz","colab_type":"code","colab":{}},"cell_type":"code","source":["# basic imports of pyTorch\n","import torch\n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","\n","# transfer learning model import\n","from torchvision import models\n","\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","print(torch.cuda.is_available())\n","\n","# some sklearn imports\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","\n","\n","# basic python imports\n","import cv2, os , sys\n","import numpy as np\n","import pandas as pd\n","import time\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tHCidGr8EnIn","colab_type":"text"},"cell_type":"markdown","source":["## torch information"]},{"metadata":{"id":"ac8zqeFDEloG","colab_type":"code","colab":{}},"cell_type":"code","source":["print(torch.__version__)\n","\n","print(torch.cuda.get_device_name(0))\n","\n","# print device properties \n","print(torch.cuda.get_device_properties(0))\n","\n","\n","# empty the cache of cuda\n","torch.cuda.empty_cache()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"JfsMjBt3L7E6","colab_type":"text"},"cell_type":"markdown","source":["# Data  Loading - and accessibility \n","Robust Class method **bold text**\n","\n","\n","\n","*   provide Train & test option.\n","*   The data is provided as generator thus saves memory.\n","* preprocessing is adjustable with ** **bold text** cv2**\n","\n","\n","\n"]},{"metadata":{"id":"HUwKG7t5MfOQ","colab_type":"code","colab":{}},"cell_type":"code","source":["class CustomedDataSet(torch.utils.data.Dataset):\n","  \n","    def __init__(self, train=True):\n","        self.train = train\n","        self.train_loc = '../input/train.csv'\n","        self.test_loc = '../input/test.csv'\n","        \n","        if self.train :\n","            trainX = pd.read_csv(self.train_loc)\n","            \n","            trainY = trainX.label.values.tolist()\n","            \n","            trainX = np.apply_along_axis(self.preprocess , axis=1 ,\\\n","                  arr = trainX.drop('label',axis=1).values.reshape(trainX.shape[0], 1, 28, 28).astype(np.uint8))\n","            \n","            # storing data to the list\n","            self.datalist = trainX\n","            self.labellist = trainY\n","       \n","        else:\n","            testX = pd.read_csv(self.test_loc)\n","            testX = testX.values.reshape(testX.shape[0], 1, 28, 28)\n","            self.datalist = testX\n","            \n","    def __getitem__(self, index):\n","        # train - provide Xtrain and Ytrain\n","        \n","        if self.train:\n","            return torch.Tensor(self.datalist[index].astype(float)), self.labellist[index]\n","        else:\n","            return torch.Tensor(self.datalist[index].astype(float))\n","    \n","    def __len__(self):\n","        return self.datalist.shape[0]\n","    \n","    def preprocess(self,img):\n","        '''\n","        Apply transformation that are not implicitly provided in the\n","        pytorch inbuilt functions.\n","        '''\n","        \n","        print(img.shape)\n","        blur = cv2.GaussianBlur(img,(3,3),0)\n","        ret3,th3 = cv2.threshold(blur,120,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n","        return img\n","      \n","      \n","   \n","        \n","# should be initialized once -or delete previos instance of \n","# object as increased in size and RAM consumption\n","\n","train_dataset = CustomedDataSet()\n","test_dataset = CustomedDataSet(train=False)\n","\n","sys.getsizeof(train_dataset)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KrnbiLmoUN35","colab_type":"text"},"cell_type":"markdown","source":["### Updated functionality of Custom data"]},{"metadata":{"id":"tlqqy7k3UMqR","colab_type":"code","colab":{}},"cell_type":"code","source":["   \n","class CustomedDataSet(torch.utils.data.Dataset):\n","    '''\n","    CustomedDataSet provide the data to be converted to\n","    train, validation and test set.\n","    num_rows = (1000,1500) - select the set of rows\n","    train = bool - composes of train and validation\n","    \n","    '''\n","    def __init__(self, train=True, num_rows = (0,10) ):\n","        self.train = train\n","        \n","        self.datalist =[]\n","        \n","        self.transform = transforms.Compose([\n","        np.uint8,\n","        transforms.ToPILImage(),\n","        transforms.Resize(size=(224,224), interpolation = 2),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406],\n","                             [0.229, 0.224, 0.225])  # Imagenet standards\n","        ])\n","        \n","        \n","        \n","        if self.train :\n","            trainX = pd.read_csv('../input/train.csv',nrows = num_rows[1])\n","            trainX = trainX.iloc[num_rows[0]:,:]\n","            trainY = trainX.label.values.tolist()\n","            arr = trainX.drop('label',axis=1).values.reshape(trainX.shape[0],  28, 28).astype(np.uint8)\n","            for i in arr:\n","                self.datalist.append(self.preprocess(i))\n","        \n","            self.datalist = torch.stack(self.datalist)\n","            self.labellist = trainY\n","            if num_rows[0] != 0:\n","                print(f\"Validation Data : {trainX.shape}\")\n","            else:\n","                print(f\"Training Data : {trainX.shape}\")\n","            \n","        else:\n","            testX = pd.read_csv('../input/test.csv', nrows = num_rows[1])\n","            testX = testX.values.reshape(testX.shape[0], 28, 28).astype(np.uint8)\n","            for i in testX:\n","                self.datalist.append(self.preprocess(i))\n","        \n","            self.datalist = torch.stack(self.datalist)\n","#             self.datalist = testX\n","            print(f\"Testing Data : {testX.shape}\")\n","            \n","    def __getitem__(self, index):\n","        if self.train:\n","            return torch.Tensor(self.datalist[index]),self.labellist[index]\n","        else:\n","            return torch.Tensor(self.datalist[index])\n","    \n","    def __len__(self):\n","        return self.datalist.shape[0]\n","    \n","    def preprocess(self,img):\n","        '''\n","        For tansfer learning apply\n","        1. gray to BGR\n","        2. transform to min crop 224\n","        3. to tensor\n","        \n","        currently implemented in this function\n","        '''\n","        img = cv2.cvtColor(img,cv2.COLOR_GRAY2BGR)\n","        img = self.transform(img)\n","        return img\n","        "],"execution_count":0,"outputs":[]},{"metadata":{"id":"hDqO6wLBOmO1","colab_type":"text"},"cell_type":"markdown","source":["## Data Augmentation - Image\n","\n","> invalid dim occur in images due to numpy and cv2 repr\n","of images\n","\n"]},{"metadata":{"id":"Lkz0yJoIL6cG","colab_type":"code","colab":{}},"cell_type":"code","source":["# Image transformations & data augmentation\n","\n","image_transforms = {\n","    # Train uses data augmentation\n","    'train':\n","    transforms.Compose([\n","        \n","        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n","        transforms.RandomRotation(degrees=15),\n","        transforms.ColorJitter(),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.CenterCrop(size=224),  # Image net standards\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406],\n","                             [0.229, 0.224, 0.225])  # Imagenet standards\n","    ]),\n","    \n","    # Validation does not use augmentation\n","    'valid':\n","    transforms.Compose([\n","        transforms.Resize(size=256), \n","        # resizes the images so the shortest side has a length of 255 pixels. \n","        # The other side is scaled to maintain the aspect ratio of the image.\n","        \n","        transforms.CenterCrop(size=224),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0xLRpC7nRBoB","colab_type":"text"},"cell_type":"markdown","source":["## Data Loading to memory"]},{"metadata":{"id":"6ycfEAquQBZf","colab_type":"code","colab":{}},"cell_type":"code","source":["# creating Data Loader\n","\n","# using CustomDataSet\n","batch_size = 32\n","\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=batch_size,\n","                                           shuffle=True,\n","                                           num_workers=2)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n","                                          batch_size=batch_size,\n","                                          shuffle=False)\n","len_train_loader = len(train_dataset)\n","\n","del train_dataset, test_dataset\n","\n","\n","# Datasets from folders & applying data_transformation\n","\n","traindir = ''\n","validdir = ''\n","\n","data = {\n","    'train':\n","    datasets.ImageFolder(root=traindir, transform=image_transforms['train']),\n","    'valid':\n","    datasets.ImageFolder(root=validdir, transform=image_transforms['valid']),\n","}\n","\n","# Dataloader iterators, make sure to shuffle\n","\n","dataloaders = {\n","    'train': DataLoader(data['train'], batch_size=batch_size, shuffle=True),\n","    'val': DataLoader(data['valid'], batch_size=batch_size, shuffle=True)\n","}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QuQ18rFmH5-P","colab_type":"text"},"cell_type":"markdown","source":["# Data Visualization"]},{"metadata":{"id":"Cea1enu6H497","colab_type":"code","colab":{}},"cell_type":"code","source":["# make iterable object to display the cluster of images\n","# within the batch\n","images, labels = iter(train_loader).next()\n","\n","# provide graid view\n","grid = torchvision.utils.make_grid(images)\n","\n","plt.imshow(grid.numpy().transpose((1, 2, 0)))\n","plt.axis('off')\n","plt.title(labels.numpy());\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FWn2FKC1H5Bb","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"xLh6RrIGSuH_","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"h2RyYMqeTMK3","colab_type":"text"},"cell_type":"markdown","source":["# Models"]},{"metadata":{"id":"qg-peL2pKxUz","colab_type":"text"},"cell_type":"markdown","source":["## Transfer learning pretrained weights\n","  \n","\n","1.   Loading Pretrained Weights\n","\n"," press **tab** after models. to get the list\n"," \n"," *All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]*\n","\n"]},{"metadata":{"id":"6tDkFljSZ3nU","colab_type":"code","colab":{}},"cell_type":"code","source":["# downloading the moadel to RAM\n","model = models.vgg16(pretrained=True)\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8kvaSAO0r_mM","colab_type":"code","colab":{}},"cell_type":"code","source":["# setting training to false\n","\n","for param in model.parameters():\n","    param.requires_grad = False\n","\n","# model.classifier\n","# model.features\n","\n","\n","''' n_classes the number of final neurons \n","(for categories -  total number of possible output)\n","'''\n","\n","model.classifier[6] = nn.Sequential(\n","                      nn.Linear(4096, 256), \n","                      nn.ReLU(), \n","                      nn.Dropout(0.4),\n","                      nn.Linear(256, n_classes),                   \n","                      nn.LogSoftmax(dim=1))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"88oSgo9sRcex","colab_type":"text"},"cell_type":"markdown","source":["![alt text](https://cdn-images-1.medium.com/max/1600/1*0W310-cMNHPWjErqPuGXpw.png)"]},{"metadata":{"id":"_sfvmfHKSAIp","colab_type":"text"},"cell_type":"markdown","source":["## Self Customizable Convolution model"]},{"metadata":{"id":"jjF7bDKzR-nt","colab_type":"code","colab":{}},"cell_type":"code","source":["class CNN(nn.Module):\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","        \n","        self.layer1 = nn.Sequential(\n","            nn.Conv2d(1 ,8, kernel_size=3,padding=2),\n","            nn.BatchNorm2d(8),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2))\n","        \n","        self.layer2 = nn.Sequential(\n","            nn.Conv2d(8, 32, kernel_size=5,padding=2),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2))\n","        \n","        self.layer3 = nn.Sequential(\n","            nn.Conv2d(32, 16, kernel_size=5,padding=2),\n","            nn.BatchNorm2d(16),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2))\n","        \n","        self.fc1 = nn.Linear(784, 128)\n","        self.fc2 = nn.Linear(128, 32)\n","        self.fc3 = nn.Linear(32, 10)\n","        \n","    def forward(self, x):\n","        out = self.layer1(x)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = out.view(out.size(0), -1)\n","        \n","        out = self.fc1(out)\n","        out = self.fc2(out)\n","        out = self.fc3(out)\n","        \n","        return out\n","\n","cnn = CNN()\n","cnn.cuda()\n","\n","# 784 the number of processed neurons can be found by\n","# doing a trial run instead of calculation"],"execution_count":0,"outputs":[]},{"metadata":{"id":"eRUBdiYBsdzy","colab_type":"code","colab":{}},"cell_type":"code","source":["# Move to gpu\n","model = model.to(device)\n","\n","\n","\n","# Distribute across 2 gpus\n","model = nn.DataParallel(model)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kObPV1YFwer6","colab_type":"code","outputId":"c6ee6ece-d588-44f1-c24e-eabe51dcbf4e","executionInfo":{"status":"ok","timestamp":1547744442221,"user_tz":-390,"elapsed":975,"user":{"displayName":"Pankaj Pundir","photoUrl":"","userId":"12951077964743328573"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["# information about training parameters\n","\n","\n","total_params = sum(p.numel() for p in model.parameters())\n","print(f'{total_params:,} total parameters.')\n","total_trainable_params = sum(\n","    p.numel() for p in model.parameters() if p.requires_grad)\n","print(f'{total_trainable_params:,} training parameters.')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["138,357,544 total parameters.\n","0 training parameters.\n"],"name":"stdout"}]},{"metadata":{"id":"CXhmNvMwX7EY","colab_type":"text"},"cell_type":"markdown","source":["# Loss and Optimizer\n"]},{"metadata":{"id":"XiBbVxRtwayT","colab_type":"code","colab":{}},"cell_type":"code","source":["criteration = nn.NLLLoss()\n","optimizer = optim.Adam(model.parameters()) # auto adjustment of learning rate\n","\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), \\\n","                            lr=learning_rate, \\\n","                            momentum=0.05)  #faster descent, avoid local minimum\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Ucl22fLvZAK2","colab_type":"text"},"cell_type":"markdown","source":["# Training Phase"]},{"metadata":{"id":"M9zXYkuQa8As","colab_type":"text"},"cell_type":"markdown","source":["## General data training"]},{"metadata":{"id":"yY6DJjvds22D","colab_type":"code","colab":{}},"cell_type":"code","source":["num_epochs = 5\n","\n","for epoch in range(num_epochs):\n","  \n","  for i, (data, targets) in enumerate(trainloader):\n","    # Generate predictions\n","    out = model(data)\n","    \n","    # Calculate loss\n","    loss = criterion(out, targets)\n","    \n","    # change gradients to 0\n","    optimizer.zero_grad()\n","    \n","    # Backpropagation\n","    loss.backward()\n","    \n","    # Update model parameters\n","    optimizer.step()\n","    \n","    # display data\n","    if (i+1) % 10 == 0:\n","      print('Loss %.4f' %( loss.data))\n","      print ('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f' %(epoch+1,\n","           num_epochs, i+1, len_train_loader//batch_size, loss.itme()))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FmBjzXeccTxR","colab_type":"text"},"cell_type":"markdown","source":["### with Early stopping method\n","*  *to avoid overfitting of data*\n","* should be used preferably\n"]},{"metadata":{"id":"JMLLZ32pxISY","colab_type":"code","colab":{}},"cell_type":"code","source":["# Early stopping details\n","\n","n_epochs_stop = 5\n","min_val_loss = np.Inf\n","epochs_no_improve = 5\n","\n","checkpoint_path = \"./\"\n","\n","# Main loop\n","for epoch in range(n_epochs):\n","  # Initialize validation loss for epoch\n","  val_loss = 0\n","  \n","  # Training loop\n","  for data, targets in trainloader:\n","    # Generate predictions\n","    out = model(data)\n","    loss = criterion(out, targets)\n","    loss.backward()\n","    optimizer.step()\n","    \n","  # Validation loop\n","  for data, targets in validloader:\n","    # Generate predictions \n","    out = model(data)\n","    loss = criterion(out, targets)\n","    val_loss += loss\n","\n","  # Average validation loss\n","  val_loss = val_loss / len(trainloader)\n","\n","  # If the validation loss is at a minimum\n","  if val_loss < min_val_loss:\n","    # Save the model\n","    torch.save(model, checkpoint_path)\n","    epochs_no_improve = 0\n","    min_val_loss = val_loss\n","\n","  else:\n","    epochs_no_improve += 1\n","    # Check early stopping condition\n","    if epochs_no_improve == n_epochs_stop:\n","      print('Early stopping!')\n","\n","      # Load in the best model\n","      model = torch.load(checkpoint_path)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ocmsqGGCbMar","colab_type":"text"},"cell_type":"markdown","source":["## image data training"]},{"metadata":{"id":"7O45QPgWbBqo","colab_type":"code","colab":{}},"cell_type":"code","source":["for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","      \n","        images = Variable(images).cuda()\n","        labels = Variable(labels).cuda()\n","\n","        optimizer.zero_grad()\n","        \n","        outputs = model(images)\n","        \n","        loss = criterion(outputs,labels)\n","        \n","        loss.backward()\n","        \n","        optimizer.step()\n","        if (i+1) % 10 == 0:\n","          print('Loss %.4f' %( loss.data))\n","          print ('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f' %(epoch+1,\n","               num_epochs, i+1, len_train_loader//batch_size, loss.itme()))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KOiTt-TciTPB","colab_type":"text"},"cell_type":"markdown","source":["# Evaluating The model"]},{"metadata":{"id":"8Eh2uzrlj3kn","colab_type":"text"},"cell_type":"markdown","source":["## general evaluation "]},{"metadata":{"id":"E7fS2GqliSMW","colab_type":"code","colab":{}},"cell_type":"code","source":["# Evaluating the model\n","model.eval()\n","counter = 0\n","\n","# Tell torch not to calculate gradients\n","with torch.no_grad():\n","    for inputs, labels in val_loader:\n","        # Move to device\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        \n","        # Forward pass\n","        output = model.forward(inputs)\n","        \n","        # Calculate Loss\n","        valloss = criterion(output, labels)\n","        \n","        # Add loss to the validation set's running loss\n","        val_loss += valloss.item()*inputs.size(0)\n","\n","        # Since our model outputs a LogSoftmax, find the real \n","        # percentages by reversing the log function\n","        output = torch.exp(output)\n","        \n","        # Get the top class of the output\n","        top_p, top_class = output.topk(1, dim=1)\n","        \n","        # See how many of the classes were correct?\n","        equals = top_class == labels.view(*top_class.shape)\n","        # Calculate the mean (get the accuracy for this batch)\n","        # and add it to the running accuracy for this epoch\n","        \n","        accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n","\n","        # Print the progress of our evaluation\n","        counter += 1\n","        print(counter, \"/\", len(val_loader))\n","\n","# Get the average loss for the entire epoch\n","train_loss = train_loss/len(train_loader.dataset)\n","valid_loss = val_loss/len(val_loader.dataset)\n","\n","# Print out the information\n","print('Accuracy: ', accuracy/len(val_loader))\n","print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch, train_loss, valid_loss))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nkXuXyuwkOBP","colab_type":"text"},"cell_type":"markdown","source":["## image evaluation"]},{"metadata":{"id":"wgfpaCUxiSWu","colab_type":"code","colab":{}},"cell_type":"code","source":["model.eval()\n","ans = torch.cuda.LongTensor()\n","\n","for images in test_loader:\n","    images = Variable(images).cuda()\n","    outputs = model(images)\n","    _,predicted = torch.max(outputs.data, 1)\n","    ans = torch.cat((ans,predicted),0)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cdXipyCKiSau","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}